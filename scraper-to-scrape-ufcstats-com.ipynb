{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7643315,"sourceType":"datasetVersion","datasetId":4014495}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install icecream\n!pip install pint\n!pip install bs4","metadata":{"_uuid":"9f44452e-4a68-4750-9430-69307358cff0","_cell_guid":"997a6c29-a674-43d1-918e-e0dd8889bb62","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-17T14:12:24.435338Z","iopub.execute_input":"2024-02-17T14:12:24.435697Z","iopub.status.idle":"2024-02-17T14:13:01.628624Z","shell.execute_reply.started":"2024-02-17T14:12:24.435657Z","shell.execute_reply":"2024-02-17T14:13:01.627219Z"},"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## This script was used to scrape the data \nfor this [UFC Fighters' Stats dataset](https://www.kaggle.com/datasets/asaniczka/ufc-fighters-statistics).\nThis was initialy a regular python script with a bunch of modules.\n\nBut I tried my best to make a it a 1 page notebook on kaggle","metadata":{}},{"cell_type":"code","source":"import string\nimport datetime\nimport re\nimport os\nimport logging\nimport time\nfrom typing import Optional, Union\n\nimport pint\nfrom bs4 import BeautifulSoup\nfrom icecream import ic\nimport requests","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:01.631484Z","iopub.execute_input":"2024-02-17T14:13:01.631959Z","iopub.status.idle":"2024-02-17T14:13:02.611276Z","shell.execute_reply.started":"2024-02-17T14:13:01.631916Z","shell.execute_reply":"2024-02-17T14:13:02.610106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper functions\nThese functions are common and reusable, so I'm not going to include them as part of the main script","metadata":{}},{"cell_type":"code","source":"def setup_logger(log_file_path: str) -> logging:\n    \"\"\"Set up a logger and return the logger instance.\n\n    Args:\n        log_file_path (str): The path of the log file.\n\n    Returns:\n        logging.Logger: The configured logger instance.\n\n    Example Usage:\n        `LOGGER = setup_logger(\"/path/to/log/file.log\")`\n\n        `LOGGER = setup_logger(log_file_path)`\n    \"\"\"\n\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)  # set the logging level to debug\n\n    log_format = logging.Formatter(\n        '%(asctime)s :   %(levelname)s   :   %(message)s')\n\n    # init the console logger\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(logging.INFO)\n    stream_handler.setFormatter(log_format)  # add the format\n    logger.addHandler(stream_handler)\n\n    return logger\n\n\ndef setup_basic_file_paths(project_name: str) -> tuple[str, str, str, str, str]:\n    \"\"\"Sets up the project folder and creates log, data, and temp folders.\n    Also creates the path to the log file.\n\n    Args:\n        project_name (str): The name of the project.\n\n    Returns:\n        tuple[str, str, str, str]: A tuple containing the paths of the project folder,\n        data folder, temp folder, log folder, and log file, respectively.\n\n    Example Usage:\n        `project_folder, data_folder, temp_folder, log_folder, log_file_path = setup_basic_file_paths(\"MyProject\")`\n    \"\"\"\n    # pylint: disable=import-outside-toplevel\n    import os\n\n    # create the project folder\n    cwd = os.getcwd()\n    project_folder = os.path.join(cwd, project_name)\n    os.makedirs(project_folder, exist_ok=True)\n\n    # create the data folder\n    data_folder = os.path.join(project_folder, 'data')\n    os.makedirs(data_folder, exist_ok=True)\n\n    # make the temp folder\n    temp_folder = os.path.join(project_folder, 'temp')\n    os.makedirs(temp_folder, exist_ok=True)\n\n    # make the log folder and log file path\n    log_folder = os.path.join(project_folder, 'logs')\n    os.makedirs(log_folder, exist_ok=True)\n    log_file_path = os.path.join(log_folder, f'{project_name}.log')\n\n    return (project_folder, data_folder, temp_folder, log_folder, log_file_path)\n\n\ndef save_temp_file(\n        temp_folder: str,\n        file_name: str,\n        content: str | set | list | dict,\n        extionsion: str) -> None:\n    \"\"\"Saves the given content to a temporary file in the specified temp folder.\n\n    Args:\n        `temp_folder (str)`: The path to the temporary folder.\n        `file_name (str)`: The name of the temporary file.\n        `content (str | set | list | dict)`: The content to be written to the temporary file. \n            Lists,sets will be formatted with newlines\n        `extension (str)`: The file extension of the temporary file.\n\n    Returns:\n        None\n\n    Example Usage:\n        `save_temp_file(\"/path/to/temp/folder\", \"example_file\", \"This is the file content\", \"txt\")`\n    \"\"\"\n    # pylint: disable=import-outside-toplevel\n    import os\n\n    # format the content to a string\n    if isinstance(content, list):\n        string_content = '\\n'.join([str(item) for item in content])\n    elif isinstance(content, set):\n        string_content = '\\n'.join([str(item) for item in content])\n    elif isinstance(content, dict):\n        # pylint: disable=import-outside-toplevel\n        import json\n        string_content = json.dumps(content)\n    else:\n        string_content = content\n\n    # now save the temp file\n    with open(os.path.join(temp_folder, f'{file_name}.{extionsion}'),\n              'w', encoding='utf-8') as temp_file:\n        temp_file.write(string_content)\n\n\ndef format_error(error: str) -> str:\n    \"\"\"Removes newlines from the given error string.\n\n    Args:\n        `error (str)`: The error string to be formatted.\n\n    Returns:\n        str: The formatted error string.\n\n    Example Usage:\n        `formatted_error = format_error(error)`\n    \"\"\"\n\n    # remove newline characters\n    error = str(error)\n    formatted_error = error.replace('\\n', '')\n\n    return formatted_error\n\n\ndef basic_request(\n        url: str,\n        logger: Optional[Union[None, logging.Logger]] = None,\n        logger_level_debug: Optional[bool] = False) -> str:\n    \"\"\"Makes a basic HTTP GET request to the given URL.\n\n    Args:\n        `url (str)`: The URL to make the request to.\n        `logger (Optional:None, logging.Logger)`: The logger instance to log warnings. \n                (default: None)\n        `logger_level_debug (Optional:bool)`: Whether to log warnings at debug level. \n                (default: False)\n\n    Returns:\n        str: The content of the response if the request was successful.\n\n    Raises:\n        RuntimeError: If the request failed after 5 retries.\n\n    Example Usage:\n        `response_content = basic_request(\"https://example.com\", logger)`\n    \"\"\"\n\n    retries = 0\n    while retries < 5:\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/119.0'\n        }\n        response = requests.get(url, headers=headers, timeout=5)\n\n        if response.status_code == 200:\n            # do the okay things\n            content = response.text\n            break\n\n        # if not okay, then start logging and retrying\n        if logger:\n            # if logger level is said to be debug, do debug, otherwise it's a warning\n            if logger_level_debug:\n                logger.warning(\n                    'Failed to get request. \\\n                    Status code %d, Response text: %s',\n                    response.status_code, format_error(response.text))\n            else:\n                logger.warning(\n                    'Failed to get request. \\\n                    Status code %d, Response text: %s',\n                    response.status_code, format_error(response.text))\n\n        # sleep 1 second and incrase retries\n        time.sleep(1)\n        retries += 1\n        continue\n\n    # raise an error if we tried more than 5 and still failed\n    if retries >= 5:\n        raise RuntimeError(f'No response from website. \\\n                            Last status code {response.status_code}, \\\n                            Response text: {format_error(response.text)}')\n\n    return content\n\n\ndef save_ndjson(data: dict, file_path: str) -> None:\n    \"\"\"Saves the given data to the same ndjson file\"\"\"\n    # pylint: disable=import-outside-toplevel\n    import json\n\n    with open(file_path, 'a', encoding='utf-8') as dump_file:\n        dump_file.write(f'{json.dumps(data)}\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.613008Z","iopub.execute_input":"2024-02-17T14:13:02.613985Z","iopub.status.idle":"2024-02-17T14:13:02.635022Z","shell.execute_reply.started":"2024-02-17T14:13:02.613939Z","shell.execute_reply":"2024-02-17T14:13:02.633659Z"},"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dev options","metadata":{}},{"cell_type":"code","source":"RUN_ONLY_ONE = False  # only extracts 1 iteraetion\nRUN_25_ITR = True # Extracts 25 fighters for testing\nIS_IC_DEBUG = False  # mark true to log ic statements","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.636371Z","iopub.execute_input":"2024-02-17T14:13:02.637037Z","iopub.status.idle":"2024-02-17T14:13:02.649298Z","shell.execute_reply.started":"2024-02-17T14:13:02.637005Z","shell.execute_reply":"2024-02-17T14:13:02.648527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Global variables","metadata":{}},{"cell_type":"code","source":"PROJECT_NAME = 'scrape_ufc_stats'\nLOGGER = None","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.651747Z","iopub.execute_input":"2024-02-17T14:13:02.652754Z","iopub.status.idle":"2024-02-17T14:13:02.660201Z","shell.execute_reply.started":"2024-02-17T14:13:02.652701Z","shell.execute_reply":"2024-02-17T14:13:02.659408Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"THis function extracts the figheter page links from a given html","metadata":{}},{"cell_type":"code","source":"def extract_fighter_pagelinks_from_serp(html: str) -> set[str]:\n\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # extract all the links\n    tags = soup.select('tr.b-statistics__table-row a')\n    links = [tag.get('href') for tag in tags]\n\n    unique_links = set(links)\n\n    return unique_links","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.661304Z","iopub.execute_input":"2024-02-17T14:13:02.662894Z","iopub.status.idle":"2024-02-17T14:13:02.672731Z","shell.execute_reply.started":"2024-02-17T14:13:02.662834Z","shell.execute_reply":"2024-02-17T14:13:02.671815Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## THis function is used to handle getting all the links to the fighers\nReturns a list of urls","metadata":{}},{"cell_type":"code","source":"def get_fighters() -> set[str]:\n    LOGGER.info('Starting to get fighters from SERPS')\n\n    # get all the letters in the alphabet\n    letters = string.ascii_lowercase\n    all_links = set()\n\n    # get the fighter pages for each letter\n    for letter in letters:\n        LOGGER.info('Extracting letter %s', letter)\n\n        # get the html for the letter\n        url = f'http://ufcstats.com/statistics/fighters?char={letter}&page=all'\n        try:\n            html = basic_request(url, LOGGER)\n        except RuntimeError:\n            continue\n\n        # now parse the html to extract the links to the fighters\n        links = extract_fighter_pagelinks_from_serp(html)\n\n        all_links.update(links)\n\n        if RUN_ONLY_ONE:\n            break\n\n    LOGGER.info('Found %d unique links', len(all_links))\n\n    return all_links","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.673975Z","iopub.execute_input":"2024-02-17T14:13:02.674862Z","iopub.status.idle":"2024-02-17T14:13:02.684553Z","shell.execute_reply.started":"2024-02-17T14:13:02.674825Z","shell.execute_reply":"2024-02-17T14:13:02.683791Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extracts the bio data from the given html","metadata":{}},{"cell_type":"code","source":"def extract_bio_data(soup: BeautifulSoup, fighter_name: str) -> dict:\n    try:\n        # convert the data card to a list and extract the data\n        physicial_data = soup.select_one(\n            '.b-list__info-box.b-list__info-box_style_small-width')\\\n            .get_text(strip=True, separator='_').split('_')\n        if IS_IC_DEBUG:\n            ic(physicial_data)\n        try:\n            height = physicial_data[1]\n        except IndexError:\n            height = None\n        try:\n            weight = physicial_data[3]\n        except IndexError:\n            weight = None\n        try:\n            reach = physicial_data[5]\n        except IndexError:\n            reach = None\n        try:\n            stance = physicial_data[7]\n        except IndexError:\n            stance = None\n        try:\n            dob = physicial_data[9]\n        except IndexError:\n            dob = None\n\n        def format_weight_to_kg(weight: str) -> int | None:\n\n            # get digits, if in pounds, convert to kg, else none\n            digit_weight = re.match(r'(\\d+)', weight).group(0)\n            if digit_weight:\n                if 'lbs' in weight:\n                    weight_in_kg = round(int(digit_weight)*0.453592, 2)\n            else:\n                weight_in_kg = None\n\n            return weight_in_kg\n\n        # format weight to kg\n        if weight and \"--\" != weight:\n            weight_in_kg = format_weight_to_kg(weight)\n        else:\n            weight_in_kg = None\n\n        # format the date of birth\n        # pylint: disable=bare-except\n        if dob and \"--\" != dob:\n            try:\n                date_of_birth = str(\n                    datetime.datetime.strptime(dob, '%b %d, %Y').date())\n            except:\n                date_of_birth = None\n        else:\n            date_of_birth = None\n\n        def convert_height_to_cm(height: str) -> int:\n            # convert height to cm\n            ureg = pint.UnitRegistry()\n            h_feet, h_inches = height.split(' ')\n\n            try:\n                # use regex to extract the digits, convert to int and store as feet and inches\n                h_feet, h_inches = int(re.match(\n                    r'(\\d+)', h_feet).group(0)), int(re.match(r'(\\d+)', h_inches).group(0))\n\n                # do conversion\n                default_height = h_feet*ureg.foot + h_inches * ureg.inch\n                height_cm = default_height.to(ureg.centimeter)\n\n                height_cm = round(height_cm.magnitude, 2)\n            except:\n                height_cm = None\n\n            return height_cm\n\n        # convert height to cm\n        if height and \"--\" != height:\n            height_cm = convert_height_to_cm(height)\n        else:\n            height_cm = None\n\n        def convert_reach_to_cm(reach: str) -> int | None:\n            reach_in_inch = reach if reach != '--' else None\n\n            if reach_in_inch:\n                reach_in_cm = int(reach_in_inch.split('\"')[0])*2.54\n            else:\n                reach_in_cm = None\n\n            return reach_in_cm\n\n        # convert reach to cm\n        if reach and \"--\" != reach:\n            reach_in_cm = convert_reach_to_cm(reach)\n        else:\n            reach_in_cm = None\n\n    # pylint: disable=broad-except\n    except Exception as error:\n        LOGGER.warning('Exception for %s on extract_bio_data(): %s', fighter_name,\n                       format_error(error))\n        height_cm = weight_in_kg = reach_in_cm = stance = date_of_birth = None\n\n    bio_data = {\n        \"height_cm\": height_cm,\n        \"weight_in_kg\": weight_in_kg,\n        \"reach_in_cm\": reach_in_cm,\n        \"stance\": stance if stance != '--' else None,\n        \"date_of_birth\": date_of_birth\n    }\n\n    return bio_data","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.68576Z","iopub.execute_input":"2024-02-17T14:13:02.686712Z","iopub.status.idle":"2024-02-17T14:13:02.702358Z","shell.execute_reply.started":"2024-02-17T14:13:02.68668Z","shell.execute_reply":"2024-02-17T14:13:02.701116Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extracts the career data from the given html","metadata":{}},{"cell_type":"code","source":"def extract_career_data(soup: BeautifulSoup, fighter_name: str) -> dict:\n    try:\n        career_data = soup.select_one(\n            '.b-list__info-box.b-list__info-box_style_middle-width')\\\n            .get_text(strip=True, separator='_').split('_')\n\n        significant_strikes_landed_per_minute = float(career_data[2])\n        significant_striking_accuracy = float(career_data[4].replace('%', ''))\n        significant_strikes_absorbed_per_minute = float(career_data[6])\n        significant_strike_defence = float(career_data[8].replace('%', ''))\n        average_takedowns_landed_per_15_minutes = float(career_data[10])\n        takedown_accuracy = float(career_data[12].replace('%', ''))\n        takedown_defense = float(career_data[14].replace('%', ''))\n        average_submissions_attempted_per_15_minutes = float(career_data[2])\n\n    # pylint: disable=broad-except\n    except Exception as error:\n        LOGGER.warning('Exception for %s on extract_career_data(): %s', fighter_name,\n                       format_error(error))\n        significant_strikes_landed_per_minute = None\n        significant_striking_accuracy = None\n        significant_strikes_absorbed_per_minute = None\n        significant_strike_defence = None\n        average_takedowns_landed_per_15_minutes = None\n        takedown_accuracy = None\n        takedown_defense = None\n        average_submissions_attempted_per_15_minutes = None\n\n    career_dict = {\n        'significant_strikes_landed_per_minute': significant_strikes_landed_per_minute,\n        'significant_striking_accuracy': significant_striking_accuracy,\n        'significant_strikes_absorbed_per_minute': significant_strikes_absorbed_per_minute,\n        'significant_strike_defence': significant_strike_defence,\n        'average_takedowns_landed_per_15_minutes': average_takedowns_landed_per_15_minutes,\n        'takedown_accuracy': takedown_accuracy,\n        'takedown_defense': takedown_defense,\n        'average_submissions_attempted_per_15_minutes': average_submissions_attempted_per_15_minutes\n    }\n\n    return career_dict","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.704019Z","iopub.execute_input":"2024-02-17T14:13:02.704584Z","iopub.status.idle":"2024-02-17T14:13:02.71907Z","shell.execute_reply.started":"2024-02-17T14:13:02.704554Z","shell.execute_reply":"2024-02-17T14:13:02.717814Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extracts fighter data from the given html","metadata":{}},{"cell_type":"code","source":"def extract_fighter_data(fighter_html: str) -> dict:\n    soup = BeautifulSoup(fighter_html, 'html.parser')\n\n    try:\n        fighter_name = soup.select_one(\n            '.b-content__title-highlight').get_text(strip=True)\n    except AttributeError:\n        fighter_name = None\n\n    try:\n        win, loss, draw = soup.select_one(\n            '.b-content__title-record')\\\n            .get_text(strip=True)\\\n            .split(' ', maxsplit=1)[-1]\\\n            .strip()\\\n            .split(' ')[0]\\\n            .strip()\\\n            .split('-')\n\n        win = int(win)\n        loss = int(loss)\n        draw = int(draw)\n\n    except (AttributeError, ValueError):\n        win, loss, draw = None, None, None\n\n    try:\n        nickname = soup.select_one('.b-content__Nickname').get_text(strip=True)\n        if \"\" == nickname:\n            nickname = None\n    except AttributeError:\n        nickname = None\n\n    bio_data = extract_bio_data(soup, fighter_name)\n    career_data = extract_career_data(soup, fighter_name)\n\n    fighter_data = {\n        \"name\": fighter_name,\n        \"nickname\": nickname,\n        \"wins\": win,\n        \"losses\": loss,\n        \"draws\": draw,\n        \"height_cm\": bio_data['height_cm'],\n        \"weight_in_kg\": bio_data['weight_in_kg'],\n        \"reach_in_cm\": bio_data['reach_in_cm'],\n        \"stance\": bio_data['stance'],\n        \"date_of_birth\": bio_data['date_of_birth'],\n        'significant_strikes_landed_per_minute': career_data['significant_strikes_landed_per_minute'],\n        'significant_striking_accuracy': career_data['significant_striking_accuracy'],\n        'significant_strikes_absorbed_per_minute': career_data['significant_strikes_absorbed_per_minute'],\n        'significant_strike_defence': career_data['significant_strike_defence'],\n        'average_takedowns_landed_per_15_minutes': career_data['average_takedowns_landed_per_15_minutes'],\n        'takedown_accuracy': career_data['takedown_accuracy'],\n        'takedown_defense': career_data['takedown_defense'],\n        'average_submissions_attempted_per_15_minutes': career_data['average_submissions_attempted_per_15_minutes'],\n\n    }\n\n    return fighter_data","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.720537Z","iopub.execute_input":"2024-02-17T14:13:02.721015Z","iopub.status.idle":"2024-02-17T14:13:02.734399Z","shell.execute_reply.started":"2024-02-17T14:13:02.720977Z","shell.execute_reply":"2024-02-17T14:13:02.733594Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the fighter page and handles extracting fighter data","metadata":{}},{"cell_type":"code","source":"def get_n_extract_fighter_data(link: str) -> dict:\n    # LOGGER.info('Processing fighter %s', link)\n\n    # first get the page\n    try:\n        page = basic_request(link, LOGGER)\n    except RuntimeError as error:\n        raise RuntimeError from error\n\n    # now parse the page:\n    fighter_dict = extract_fighter_data(page)\n\n    return fighter_dict","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.735855Z","iopub.execute_input":"2024-02-17T14:13:02.736177Z","iopub.status.idle":"2024-02-17T14:13:02.748863Z","shell.execute_reply.started":"2024-02-17T14:13:02.736149Z","shell.execute_reply":"2024-02-17T14:13:02.747795Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## This is the main handler for this script","metadata":{}},{"cell_type":"code","source":"def executor() -> None:\n    # pylint: disable=global-statement\n    global LOGGER\n\n    # setup folders\n    _, data_folder, _, _, log_file_path = setup_basic_file_paths(\n        PROJECT_NAME)\n\n    # setup a logger\n    LOGGER = setup_logger(log_file_path)\n\n    fighter_links = get_fighters()\n\n    # ndjson save file for all the fighteres\n    ndjson_file_path = os.path.join(data_folder, 'fighter_data.ndjson')\n\n    # get data for each individual fighter\n    for i, fighter in enumerate(fighter_links):\n        LOGGER.info('Processing %d out of %d. Fighter url: %s',\n                    i, len(fighter_links), fighter)\n        try:\n            fighter_data = get_n_extract_fighter_data(fighter)\n\n            save_ndjson(fighter_data, ndjson_file_path)\n\n            if RUN_ONLY_ONE:\n                break\n            if RUN_25_ITR and i==24:\n                break\n        except RuntimeError:\n            LOGGER.debug(\n                'Skipping a fighter due to a requests error: %s', fighter)\n\n            if RUN_ONLY_ONE:\n                break\n            if RUN_25_ITR and i==24:\n                break\n            continue\n    LOGGER.info('Finished the executor')","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.750374Z","iopub.execute_input":"2024-02-17T14:13:02.750759Z","iopub.status.idle":"2024-02-17T14:13:02.75895Z","shell.execute_reply.started":"2024-02-17T14:13:02.750706Z","shell.execute_reply":"2024-02-17T14:13:02.758103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"executor()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T14:13:02.760062Z","iopub.execute_input":"2024-02-17T14:13:02.760819Z","iopub.status.idle":"2024-02-17T14:13:45.041949Z","shell.execute_reply.started":"2024-02-17T14:13:02.760789Z","shell.execute_reply":"2024-02-17T14:13:45.040778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## This script only ran 25 iteration\nSince I already have the data extracted. \n\nFeel free to use this script to learn how to create a scraper","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.giphy.com/5K7ngCtszoxxbaBieC.gif\" style=\"margin:auto;width:40%;height:auto;\">","metadata":{}}]}